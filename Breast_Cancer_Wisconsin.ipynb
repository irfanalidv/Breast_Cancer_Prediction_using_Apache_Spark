{"cells":[{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom pyspark.sql.types import DoubleType\nfrom pyspark.sql.types import LongType\nfrom pyspark.sql.types import FloatType\nfrom pyspark.sql.functions import sum\nfrom pyspark.sql.functions import array\nfrom pyspark.sql.functions import udf\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer\nfrom pyspark.ml.feature import VectorIndexer, VectorAssembler\nfrom pyspark.ml.feature import StandardScaler\nfrom pyspark.ml.feature import VectorSlicer\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline, PipelineModel\n"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["#load data as dataframe\ndata = sqlContext.read.load('/FileStore/tables/Breast_Cancer_Wisconsin/data.csv', format='csv', header='true', inferSchema='true')\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["\n#keep only the features with average values. don't keep id either\ndata = data.select(['diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', \n                    'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', \n                    'fractal_dimension_mean'])\n\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#display the structure\ndata.printSchema()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["#randomly split the data into a test/train set and validation set (80%/20%)\nsplits = data.randomSplit([0.8, 0.2])\ndata_train = splits[0]\ndata_val = splits[1]\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["display(data_train)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["#take a look at some data\ndata.limit(5).toPandas()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#convert diagnosis --> diagnosis_numeric. no one-hot encoding required because this can only have \n#two values (benign or malignant)\nstringIndexerDiagnosis = StringIndexer(inputCol='diagnosis', outputCol='label')"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["all_feats = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', \n             'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\nassemblerAllFeatures = VectorAssembler(inputCols=all_feats, outputCol='features')"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["\n#define the pipeline\npipeline = Pipeline(stages=[stringIndexerDiagnosis, assemblerAllFeatures])\n"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["\n#fit the pipeline on the training data. what it means to \"fit\" the pipeline is to basically go through each of the \n#stages of the piepline and figure out the appropriate parameters. for example, in the stringIndexerDiagnosis stage, \n#it assigns benign=0 and malignant=1 (or vice versa)\npipelineModel = pipeline.fit(data_train)\n\n"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["#actually transform the data now that the pipeline is fit. note that the resultant features vector may be sparse\noutput = pipelineModel.transform(data_train)\n"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["#take a look at the output\noutput.limit(3).toPandas()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["#It's generally a good idea to take a look at the correlations between input features. This is a good first step at feature selection, in which we can #remove any highly correlated (and therefore redundant) features.\n#To do correlation analysis, Let's select all the features and conver to a Pandas DataFrame.\n#note: if the data set is large, we should limit the number of samples before converting to pandas\ncont_feats = data_train.select(all_feats).toPandas()\n"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["#compute the correlation matrix\ncorr = cont_feats.corr()\nprint 'correlation matrix:'\ncorr\n#fractal_dimension_mean  | smoothness_mean  | "],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["#plot the correlation matrix as a heatmap \nf, ax = plt.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, cmap=cmap, ax=ax)\nplt.show()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["#exclude the highly correlated features\nall_feats = ['texture_mean', 'smoothness_mean', 'compactness_mean',  'symmetry_mean', 'fractal_dimension_mean']\ncont_feats = data_train.select(all_feats).toPandas()\n\n#compute the correlation matrix\ncorr = cont_feats.corr()\nprint 'correlation matrix:'\ncorr"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n\n#the following lines allow the notebook to have multiple outputs for a single cell\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'\n\n#plot the correlation matrix as a heatmap \nf, ax = plt.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, cmap=cmap, ax=ax)\nplt.show()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["#Now, let's put everything together. Define a new feature assembler that only assembles the non-correlated features. Define a random forest classifier with default parameters, and put everything together in a pipeline.\n#define a new assembler on only the non-correlated features\nassemblerAllFeatures = VectorAssembler(inputCols=all_feats, outputCol='features')\n\nmodel = RandomForestClassifier()\n\n#chain everything together into a pipeline\npipeline = Pipeline(stages=[stringIndexerDiagnosis, assemblerAllFeatures, model])\n\n"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["#Note that we didn't define any of the parameters for the random forest classifier. As it turns out, there are two parameters that need to be optimized: the max depth of the trees, and the number of trees in the random forest. To identify the optimal values for these parameters,we will use the CrossValidator.\n\n#we will set up the max depth to range from 1 to 10, and the number of trees to range from 5 to 70 in increments of 5. The metric used to determine the best performing model will be the area under the ROC, and the number of folds of cross validation are set to 3.\n\n#define a ParamGridBuilder to determine optimal values of elasticNetParam (range [0,1]) and regParam (typically \n#ranges between 0 and 1). we'll sweep through 0 to 1 in increments of 0.1 for both parameters\n\nparamGrid = ParamGridBuilder().addGrid(model.maxDepth, range(1,11)) \\\n                              .addGrid(model.numTrees, range(5,75,5)).build()\nparamGrid"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["#define the RMSE to be the evaluation metric for model performance\n#root-mean-square deviation\nevaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["#set up 3-fold cross validation to determine the optimal depth parameter (this can be set higher for potentially \n#better results)\ncrossval = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\n\n"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["#do the actual cross validation on the training data\nCV_model = crossval.fit(data_train)\n"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["#get the best model\nbest_model = CV_model.bestModel\n\n#for the best model, print out each tree in the random forest and list its depth and number of nodes\nprint ('number of trees: %i') % (len(best_model.stages[-1].trees))\nbest_model.stages[-1].trees"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["#From the above output, we can see that the optimal max depth was identified to be 6 and the number of trees used was 35."],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":26}],"metadata":{"name":"Breast_Cancer_Wisconsin","notebookId":2929498337745984},"nbformat":4,"nbformat_minor":0}
